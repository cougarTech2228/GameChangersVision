{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "former-check",
   "metadata": {},
   "source": [
    "# 2228 CougarTech Galactic Search Neural Network\n",
    "\n",
    "## Overview\n",
    "This notebook will compile and train a MobileNetV2 based Convolutional Neural Network (CNN) to detect the 4 possible field layouts for the 2021 Galactic Search autonomous challange.\n",
    "\n",
    "Using training images captured of all the various field layouts from different possible robot angles and perspectives, the model will be trained and saved off to a Tensorflow .pb file that can be loaded onto the Raspberry Pi and opened up using OpenCV 4.5.1\n",
    "\n",
    "## Training the Model\n",
    "All of the training, validation, and test images must be in the following directory structure:\n",
    "```\n",
    "  - data\n",
    "   `- GalacticSearch\n",
    "     |- test\n",
    "     | |- BlueA\n",
    "     | |- BlueB\n",
    "     | |- RedA\n",
    "     | `- RedB\n",
    "     |- train\n",
    "     | |- BlueA\n",
    "     | |- BlueB\n",
    "     | |- RedA\n",
    "     | `- RedB\n",
    "     `- valid\n",
    "       |- BlueA\n",
    "       |- BlueB\n",
    "       |- RedA\n",
    "       `- RedB\n",
    "```\n",
    "- **Train** images are the actual images fed into the network durring it's training process. The more images you have here, the more reliable your network will end up.\n",
    "- **Valid** images are the images used for network validation durring the training process. These images should be separate from the training images, as the point is to feed the network images it has never seen before to determine how well it can gerneralze input.\n",
    "- **Test** images are like the validation images, but a smaller sub-set used to perform one final test of the network to determine how well it performs\n",
    "\n",
    "To perform the actual model training, simply run this notebook. It will result in the compiled model being written to the `models` folder\n",
    "\n",
    "## Deploying\n",
    "The Raspberry Pi wpilibpi vision application must be set to \"custom\" instead of \"uploaded Java jar\" or any other option. The actual copying of the vision code, and the required libraries and CNN model will be performed by gradle.\n",
    "\n",
    "Note: The stock wpilibpi image only supports OpenCV 3.4.7, so a custom build of 4.5.1 was made, and will be deployed to the Pi automatically\n",
    "\n",
    "To deploy all required components to the Pi, and automatically restart the vision service, run the following commands from VSCode:\n",
    "- `./gradlew build` -- Build the actual Java application\n",
    "- `./gradlew deploy` -- Deploy everything to the Pi\n",
    "\n",
    "This will do the following:\n",
    "- Check for the presence of `/home/pi/opencv-4.5.1`. If this does not exist, it will copy over the opencv-4.5.1 distribution and extract it to disk\n",
    "- Check for the presence of the CNN .pb file, if not found, it will copy it to `/home/pi/`\n",
    "- Copy the latest compiled .jar file, and the runCamera command to `/home/pi/`\n",
    "- Restart the camera service, causing the updated code to start running\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-Level imports and definition of some helper fuction\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the training data\n",
    "train_path = 'data/GalacticSearch/train'\n",
    "valid_path = 'data/GalacticSearch/valid'\n",
    "test_path = 'data/GalacticSearch/test'\n",
    "height=224\n",
    "width=224\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) \\\n",
    "    .flow_from_directory(directory=train_path, target_size=(width,height), classes=['RedA', 'RedB', 'BlueA', 'BlueB'], batch_size=10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) \\\n",
    "    .flow_from_directory(directory=valid_path, target_size=(width,height), classes=['RedA', 'RedB', 'BlueA', 'BlueB'], batch_size=10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) \\\n",
    "    .flow_from_directory(directory=test_path, target_size=(width,height), classes=['RedA', 'RedB', 'BlueA', 'BlueB'], batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check to make sure our training data is what we think it should be\n",
    "imgs, labels = next(train_batches)\n",
    "plotImages(imgs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-translator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define and compile the model.\n",
    "# We are using the pre-defined MobileNetV2 model as it's a well-established model\n",
    "# That is designed to run on mobile devices with less-power CPUs, like the\n",
    "# Raspberry Pi\n",
    "\n",
    "# MobileNetV2 is defined as taking in a 224x224x3 RGB Matrix, and classifying\n",
    "# the image into one of 1000 caregories. As we only need 4 gategories for\n",
    "# Galactic search, we'll strip off the last 1000-node clasification layer,\n",
    "# and add in a few more layers ending with a softmax classification layer for\n",
    "# 4 labels.\n",
    "\n",
    "# Imports the MobileNetV2 model and discards the last 1000 neuron layer.\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    weights='imagenet',include_top=False) \n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "\n",
    "# we add dense layers so that the model can learn more complex functions\n",
    "# and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x)\n",
    "\n",
    "# dense layer 2\n",
    "x=Dense(1024,activation='relu')(x)\n",
    "\n",
    "# dense layer 3\n",
    "x=Dense(512,activation='relu')(x)\n",
    "\n",
    "# final layer with softmax activation for 4 classes\n",
    "preds=Dense(4,activation='softmax')(x) \n",
    "\n",
    "# specify the inputs and outputs\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "# Mark the first 20 layers as Frozen, as we want to use\n",
    "# what the default model already knows\n",
    "for layer in model.layers[:20]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "# print it out so we can see what we're dealing with\n",
    "model.summary()\n",
    "\n",
    "# we need to compile the model before we can use it. We're using pretty\n",
    "# standard optimizers and loss calculation methods here\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the actual learning step\n",
    "model.fit(x=train_batches,\n",
    "          steps_per_epoch=len(train_batches),\n",
    "          validation_data=valid_batches,\n",
    "          validation_steps=len(valid_batches),\n",
    "          epochs=20,\n",
    "          verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-inside",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the test data through and plot the results\n",
    "\n",
    "test_imgs, test_labels = next(test_batches)\n",
    "# plotImages(test_imgs)\n",
    "# print(test_labels)\n",
    "\n",
    "# Do the actual predictions\n",
    "predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)\n",
    "\n",
    "print (predictions)\n",
    "# round the probabilities to 0-1\n",
    "np.round(predictions)\n",
    "\n",
    "# print out the indcies of each class label in the test data\n",
    "# we need to make sure the order here matches the order in the\n",
    "# cm_plot_labels below\n",
    "test_batches.class_indices\n",
    "\n",
    "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))\n",
    "\n",
    "cm_plot_labels = ['RedA','RedB', 'BlueA', 'BlueB']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will write out the trained model in a tensorflow format (.pb)\n",
    "# so that it can be read in by OpenCV in the actual vision code\n",
    "# running on the Pi\n",
    "\n",
    "# Directory name to save the model to\n",
    "frozen_out_path = 'models'\n",
    "\n",
    "# Name of the .pb file\n",
    "frozen_graph_filename = 'galactic_search'\n",
    "\n",
    "# Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype))\n",
    "\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "frozen_func.graph.as_graph_def()\n",
    "\n",
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                  logdir=frozen_out_path,\n",
    "                  name=f\"{frozen_graph_filename}.pb\",\n",
    "                  as_text=False)\n",
    "\n",
    "# tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "#                   logdir=frozen_out_path,\n",
    "#                   name=f\"{frozen_graph_filename}.pbtxt\",\n",
    "#                   as_text=True)\n",
    "\n",
    "# Also save a keras formatted version of the model so that we\n",
    "# can re-import it here if we want later\n",
    "model.save(os.path.join(frozen_out_path, frozen_graph_filename + \".h5\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
